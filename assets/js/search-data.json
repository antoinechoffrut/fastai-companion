{
  
    
        "post0": {
            "title": "Computing gradients and derivatives in PyTorch",
            "content": ". tags: mathematics pytorch gradients backward automatic differentiation vector-Jacobian product backpropagation . . tl;dr . The backward function in PyTorch can be used to compute the derivatives or gradients of functions. The backward function computes vector-Jacobian products so that the appropriate vector must be determined. In other words, the correct gradient argument must be passed to backward, although not passing gradient explicitly will cause backward to choose the appropriate value but only in the simplest cases. . This notebook explains vector-Jacobian products and how to choose the gradient argument in the backward function in the general case. . A brief overview . In the case of a function taking a scalar and returning a scalar, the use of the backward function is quite straight-forward: . # collapse-hide import torch x = torch.tensor(1., requires_grad=True) y = x**2 y.backward() print(f&quot;Derivative at a single point:&quot;) print(x.grad.data) . . Derivative at a single point: tensor(2.) . However, when . the function is multi-valued (e.g. vector- or matrix-valued); or | one wishes to compute the derivative of a function at mulitple points, | . then the gradient argument in backward must be suitably chosen. For example: . # collapse-hide import torch x = torch.linspace(-2, 2, 5, requires_grad=True) y = x**2 gradient = torch.ones_like(y) y.backward(gradient) print(&quot;Derivative at multiple points:&quot;) print(x.grad.data) . . Derivative at multiple points: tensor([-4., -2., 0., 2., 4.]) . Indeed, more precisely, the backward function computes vector-Jacobian products, which is not explicit in the function&#39;s doc string: . # collapse-hide print(&quot;First line of `torch.Tensor.backward` doc string:&quot;) print(&quot; &quot;&quot;+ torch.Tensor.backward.__doc__.split(&quot; n&quot;)[0] + &quot; &quot;&quot;) . . First line of `torch.Tensor.backward` doc string: &#34;Computes the gradient of current tensor w.r.t. graph leaves.&#34; . although some explanations are given in this official tutorial. The crucial point is therefore to choose the appropriate vector, which is passed to the backward function in its gradient argument: . # collapse-hide import inspect import torch print(f&quot;torch.Tensor.backward{inspect.signature(torch.Tensor.backward)}&quot;) print(&quot;...&quot;) print(&quot; n&quot;.join(torch.Tensor.backward.__doc__.split(&quot; n&quot;)[11:18])) print(&quot;...&quot;) . . torch.Tensor.backward(self, gradient=None, retain_graph=None, create_graph=False) ... Arguments: gradient (Tensor or None): Gradient w.r.t. the tensor. If it is a tensor, it will be automatically converted to a Tensor that does not require grad unless ``create_graph`` is True. None values can be specified for scalar Tensors or ones that don&#39;t require grad. If a None value would be acceptable then this argument is optional. ... . There is a way around specifying the gradient argument. Revisiting the example above, the derivative at multiple points can be equivalently calculated by adding a sum(): . # collapse-hide import torch x = torch.linspace(-2, 2, 5, requires_grad=True) y = (x**2).sum() y.backward() print(&quot;Derivative at multiple points:&quot;) print(x.grad.data) . . Derivative at multiple points: tensor([-4., -2., 0., 2., 4.]) . Here, the backward method is invoked on a different tensor: . (x**2).backward() . if x contains a single input, vs . (x**2).sum().backward() . if x contains multiple inputs. . On the other hand, passing the gradient argument, whether x contains one or multiple inputs, the same command is used to compute the derivatives: . y = (x**2) y.backward(torch.ones_like(y)) . Roughly speaking, the difference between the two methods, namely setting gradient=torch.ones_like(y) or adding sum(), is in the order of the summation and differentiation. . Usage examples of the backward function . The derivative of the scalar, univariate function $f(x)=x^2$ at a single point $x=1$: . import torch x = torch.tensor(1., requires_grad=True) y = x**2 y.backward() x.grad . tensor(2.) . The derivative of the scalar, univariate function $f(x)=x^2$ at multiple points $x= -2, -1, dots, 2$: . import torch x = torch.linspace(-2, 2, 5, requires_grad=True) y = x**2 v = torch.ones_like(y) y.backward(v) x.grad . tensor([-4., -2., 0., 2., 4.]) . The gradient of the scalar, multivariate function $f(x_1, x_2)=3x_1^2 + 5x_2^2$ at a single point $(x_1, x_2)=(-1, 2)$: . import torch x = torch.tensor([-1., 2.], requires_grad=True) w = torch.tensor([3., 5.]) y = (x*x*w).sum() y.backward() x.grad . tensor([-6., 20.]) . The gradient of the scalar, multivariate function $f(x_1, x_2) = -x_1^2 + x_2^2$ at multiple points $(x_1, x_2)$: . import torch x = torch.arange(6, dtype=float).view(3, 2).requires_grad_(True) w = torch.tensor([-1, 1]) y = (x*x*w).sum(1) v = torch.ones_like(y) y.backward(v) x.grad . tensor([[-0., 2.], [-4., 6.], [-8., 10.]], dtype=torch.float64) . The derivatives of the vector-valued, univariate function $f(x)= (-x^3, 5x)$ at a single point $x=1$, i.e. the derivative of . its first component function $f_1(x)=-x^3$; and | its second component function $f_2(x)=5x$. | . # collapse-hide import torch x = torch.tensor(1., requires_grad=True) y = torch.stack([-x**3, 5*x]) v1 = torch.tensor([1., 0.]) y.backward(v1, retain_graph=True) print(f&quot;f_1&#39;({x.data.item()}) = {x.grad.data.item():&gt;4}&quot;) x.grad.zero_() v2 = torch.tensor([0., 1.]) y.backward(v2) print(f&quot;f_2&#39;({x.data.item()}) = {x.grad.data.item():&gt;4}&quot;) . . f_1&#39;(1.0) = -3.0 f_2&#39;(1.0) = 5.0 . The derivatives of the vector-valued, univariate function $f(x)= (-x^3, 5x)$ at multiple points, i.e. the derivative of . its first component function $f_1(x)=-x^3$; and | its second component function $f_2(x)=5x$. | . # collapse-hide import torch import itertools x = torch.arange(3, dtype=float, requires_grad=True) y = torch.stack([-x**3, 5*x]) ranges = [range(_) for _ in y.shape] v1 = torch.tensor([1. if i == 0 else 0. for i, j in itertools.product(*ranges)]).view(*y.shape) y.backward(v1, retain_graph=True) print(f&quot;Derivative of f_1(x)=-3x^2 at the points {tuple(x.data.view(-1).tolist())}:&quot;) print(x.grad) x.grad.zero_() v2 = torch.tensor([1. if i == 1 else 0. for i, j in itertools.product(*ranges)]).view(*y.shape) y.backward(v2) print(f&quot; nDerivative of f_2(x)=5x at the points {tuple(x.data.view(-1).tolist())}:&quot;) print(x.grad) . . Derivative of f_1(x)=-3x^2 at the points (0.0, 1.0, 2.0): tensor([ 0., -3., -12.], dtype=torch.float64) Derivative of f_2(x)=5x at the points (0.0, 1.0, 2.0): tensor([5., 5., 5.], dtype=torch.float64) . The gradients of the vector-valued, multivariate function $$ f(x_1, dots, x_n) = (x_1 + dots + x_n ,, x_1^2 + dots + x_n^2) $$ at a single point $(x_1, dots, x_n)$, i.e. the gradient of . its first component function $f_1(x_1, dots, x_n) = x_1 + dots + x_n$; and | its second component function $f_2(x_1, dots, x_n) = x_1^2 + dots + x_n^2$. | . # collapse-show import torch x = torch.arange(4, dtype=float, requires_grad=True) y = torch.stack([x.sum(), (x**2).sum()]) print(f&quot;x : {tuple(x.data.tolist())}&quot;) print(f&quot;y = (y_1, y_2) : {tuple(y.data.tolist())}&quot;) v1 = torch.tensor([1., 0.]) y.backward(v1, retain_graph=True) print(f&quot;gradient of y_1 : {tuple(x.grad.data.tolist())}&quot;) x.grad.zero_() v2 = torch.tensor([0., 1.]) y.backward(v2) print(f&quot;gradient of y_2 : {tuple(x.grad.data.tolist())}&quot;) . . x : (0.0, 1.0, 2.0, 3.0) y = (y_1, y_2) : (6.0, 14.0) gradient of y_1 : (1.0, 1.0, 1.0, 1.0) gradient of y_2 : (0.0, 2.0, 4.0, 6.0) . The gradients of the vector-valued, multivariate function $$ f(x_1, dots, x_n) = (x_1 + dots + x_n ,, x_1^2 + dots + x_n^2) $$ at multiple points, i.e. the gradient of . its first component function $f_1(x_1, dots, x_n) = x_1 + dots + x_n$; and | its second component function $f_2(x_1, dots, x_n) = x_1^2 + dots + x_n^2$. | . # collapse-show import torch import itertools x = torch.arange(4*3, dtype=float).view(-1,4).requires_grad_(True) y = torch.stack([x.sum(1), (x**2).sum(1)]) print(&quot;x:&quot;) print(x.data) print(&quot;y:&quot;) print(y.data) print() ranges = [range(_) for _ in y.shape] v1 = torch.tensor([1. if i == 0 else 0. for i, j in itertools.product(*ranges)]).view(*y.shape) y.backward(v1, retain_graph=True) print(&quot;Gradients of the f1 at multiple points:&quot;) print(x.grad) x.grad.zero_() print() v2 = torch.tensor([1. if i == 1 else 0. for i, j in itertools.product(*ranges)]).view(*y.shape) y.backward(v2) print(&quot;Gradients of the f2 at multiple points:&quot;) print(x.grad) . . x: tensor([[ 0., 1., 2., 3.], [ 4., 5., 6., 7.], [ 8., 9., 10., 11.]], dtype=torch.float64) y: tensor([[ 6., 22., 38.], [ 14., 126., 366.]], dtype=torch.float64) Gradients of the f1 at multiple points: tensor([[1., 1., 1., 1.], [1., 1., 1., 1.], [1., 1., 1., 1.]], dtype=torch.float64) Gradients of the f2 at multiple points: tensor([[ 0., 2., 4., 6.], [ 8., 10., 12., 14.], [16., 18., 20., 22.]], dtype=torch.float64) . Mathematical preliminaries . Scalars, vectors, matrices, and tensors . A scalar is a real number. It is usually denoted with $x$. | An $n$-dimensional vector is a list $(x_1, dots, x_n)$ of scalars. | An $m$-by-$n$ matrix is an array with $m$ rows and $n$ columns of scalars: $$ begin{bmatrix}w_{1,1}&amp; dots&amp;w_{1,n} vdots&amp; ddots&amp; vdots w_{m,1}&amp; dots&amp;w_{m,n} end{bmatrix} $$ | A column vector of length $n$ is a $n$-by-$1$ matrix: $$ begin{bmatrix}x_1 vdots x_n end{bmatrix}$$ Note that it is distinct from its vector counterpart $(x_1, dots, x_n)$. | A row vector of length $n$ is a $1$-by-$n$ matrix: $$ begin{bmatrix}x_1&amp; dots&amp;x_n end{bmatrix}$$ Note that it is distinct from its vector and column vector counterparts. | . . Note: For convenience, we may denote a vector, a column vector, or a row vector with a single symbol, typically $x$. . In another post we establish the following correspondence between these mathematical entities and their tensor counterparts in PyTorch: . mathematical name mathematical notation tensor shape tensor dimension . scalar | $x$ | () | 0 | . vector | $(x_1, dots, x_n)$ | (n,) | 1 | . matrix | $ begin{bmatrix}w_{1,1}&amp; dots&amp;w_{1,n} vdots&amp; ddots&amp; vdots w_{m,1}&amp; dots&amp;w_{n,m} end{bmatrix}$ | (m,n) | 2 | . column vector | $ begin{bmatrix}x_1 vdots x_n end{bmatrix}$ | (n,1) | 2 | . row vector | $ begin{bmatrix}x_1&amp; dots&amp;x_n end{bmatrix}$ | (1,n) | 2 | . Mathematical functions . We consider functions which are mappings from scalars, vectors, or matrices to scalars, vectors, or matrices. It is generically denoted $y=f(x)$. | A scalar function $y=f(x)$ is a function returning a scalar, i.e. $y$ is a scalar. | A vector-valued function $y=f(x)$ is a function returning a vector, i.e. $y$ is a vector. We often write $$f(x) = (f_1(x), dots, f_m(x))$$ if the output is $m$-dimensional, where each of $f_1(x), dots, f_m(x)$ is a scalar function. | A univariate function $y=f(x)$ is a function depending on a scalar $x$. | A multivariate function $y=f(x)$ is a function depending on a vector $x=(x_1, dots, x_n)$. | . In summary . $y=f(x)$ scalar-valued vector-valued . univariate | $x$ is a scalar$y$ is a scalar | $x$ is a scalar$y$ is a vector | . multivariate | $x$ is a vector$y$ is a scalar | $x$is a vector$y$ is a vector | . Differentiation . Basic definitions . We do not recall the definitions for: . the derivative $f&#39;(x)$ of a scalar, uni-variate function $y=f(x)$ evaluated at a scalar $x$; | the partial derivatives $ frac{ partial f}{ partial x_i}(x)$, $i=1, dots, n$, of a scalar, multivariate function $y=f(x)$ with respect to the variables $x_1, dots, x_n$, and evaluated at $x=(x_1, dots, x_n)$. | . Derivatives of vector-valued, univariate functions . The derivative of a vector-valued, uni-variate function $y=f(x)$ evaluated at a scalar $x$ is the vertical concatenation of the derivatives of its component functions: $$f&#39;(x) = begin{bmatrix}f_1&#39;(x) vdots f_m&#39;(x) end{bmatrix}$$ . Gradients . The gradient of a scalar-valued function $y=f(x)$, is the row vector of its partial derivatives: $$ nabla f(x) = begin{bmatrix} frac{ partial f}{ partial x_1}(x)&amp; dots&amp; frac{ partial f}{ partial x_n}(x) end{bmatrix}$$ with length $n$ if $x$ is $n$-dimensional: $x=(x_1, dots, x_n)$. . Jacobians . The Jacobian of a vector-valued, multivariate function $y=f(x)$ is the vertical concatenation of the gradients of the component functions $f_1, dots, f_m$: $$J_f(x) ,= , begin{bmatrix} nabla f_1(x) vdots nabla f_m(x) end{bmatrix} ,= , begin{bmatrix} frac{ partial f_1}{ partial x_1}(x)&amp; dots&amp; frac{ partial f_1}{ partial x_n}(x) vdots&amp; ddots&amp; vdots frac{ partial f_m}{ partial x_1}(x)&amp; dots&amp; frac{ partial f_m}{ partial x_n}(x) end{bmatrix} $$ It is thus an $m$-by-$n$ matrix, i.e. with $m$ rows and $n$ columns. . Special case: $m=1$ . In case $m=1$, the Jacobian agrees with the gradient of a scalar, multivariate function: $$J_f(x) = nabla f(x)$$ . Special case: $n=1$ . In case $n=1$, the Jacobian agrees with the derivative of a vector-valued, univariate function. $$J_f(x) = begin{bmatrix}f_1&#39;(x) vdots f_m&#39;(x) end{bmatrix}$$ . Vector-Jacobian products . Given a vector-valued, multivariate function $y=f(x)$ and a column vector $v= begin{bmatrix}v_1 vdots v_m end{bmatrix}$, the vector-Jacobian product is the matrix multiplication $$v^ top J_f(x) ,= , begin{bmatrix} v_1&amp; dots&amp;v_m end{bmatrix} begin{bmatrix} frac{ partial f_1}{ partial x_1}(x)&amp; dots&amp; frac{ partial f_1}{ partial x_n}(x) vdots&amp; ddots&amp; vdots frac{ partial f_m}{ partial x_1}(x)&amp; dots&amp; frac{ partial f_m}{ partial x_n}(x) end{bmatrix} $$ which is then a row vector of length $n$. . Special case . If $v^ top$ happens to be the gradient of a scalar-valued function $z= ell(y)$ evaluated at $f(x)$, i.e. $v = nabla ell(y)$ where $y=f(x)$, then begin{equation} v^ top J_f(x) ,= , nabla ( ell circ f)(x) end{equation} In other words, $v^ top J_f(x)$ is the gradient of the composition of the function $ ell$ with the function $f$. . Note: The vector-Jacobian product can be generalized to cases where $x$ and $y$ are (mathematical) tensors of higher dimensions. This generalization is in fact used in some of the examples of this post. . Application: Gradients of vector-valued functions . If $y=f(x)=(f_1(x), dots, f_m(x))$ is a vector-valued, multivariate function, one computes the gradients $ nabla f_1(x), dots, nabla f_m(x)$ one at a time, each time with a suitable vector $v$. Indeed, fix $i$ between $1$ and $m$, and define $ ell_i(y)=y_i$ the function selecting the $i$-th coordinate of $y=(y_1, dots, y_m)$, so that $$f_i(x) = ell_i(f(x)) ,.$$ Noting that $$ nabla ell_i(y) = begin{bmatrix}0&amp; cdots&amp;0&amp;1&amp;0&amp; cdots&amp;0 end{bmatrix}$$ where the only non-zero coordinate is in the $i$-th position, then $$ begin{align} nabla ell_i(f(x))J_f(x) &amp; = begin{bmatrix}0&amp; cdots&amp;0&amp;1&amp;0&amp; cdots&amp;0 end{bmatrix} begin{bmatrix} frac{ partial f_1}{ partial x_1}(x)&amp; dots&amp; frac{ partial f_1}{ partial x_n}(x) vdots&amp; ddots&amp; vdots frac{ partial f_m}{ partial x_1}(x)&amp; dots&amp; frac{ partial f_m}{ partial x_n}(x) end{bmatrix} &amp;= begin{bmatrix} frac{ partial f_i}{ partial x_1}(x)&amp; dots&amp; frac{ partial f_i}{ partial x_n}(x) end{bmatrix} end{align} $$ . Application: Derivatives at multiple points . To evaluate the derivative of a scalar, univariate function $f(x)$ at multiple sample points $x^{(1)}, dots, x^{(N)}$, we create a new, vector-valued and multivariate function $$F(x)= begin{bmatrix}f left(x^{(1)} right) vdots f left(x^{(N)} right) end{bmatrix} qquad textrm{where} qquad x ,= ,(x^{(1)}, dots, x^{(N)}) ,.$$ Thus, its Jacobian is $$J_F(x)= begin{bmatrix} f&#39;(x^{(1)})&amp;&amp;&amp;&amp; &amp; ddots&amp;&amp;&amp; &amp;&amp;f&#39;(x^{(j)})&amp;&amp; &amp;&amp;&amp; ddots&amp; &amp;&amp;&amp;&amp;f&#39;(x^{(N)}) end{bmatrix} $$ where all off-diagonal terms are $0$. Thus, setting $v= begin{bmatrix}1 vdots 1 end{bmatrix}$, we obtain the gradient of $f$ evaluated at the $N$ sample points $x^{(1)} ,, dots ,, x^{(N)}$: $$ begin{bmatrix}f&#39;(x^{(1)})&amp; dots&amp; f&#39;(x^{(j)})&amp; cdots&amp; f&#39;(x^{(N)}) end{bmatrix} = left[1 ,, dots ,,1 right] J_f(x) ,.$$ The interpretation here is that the resulting row vector contains the derivative of $f$ at the samples $x^{(1)}$ to $x^{(N)}$. . The trick with sum() . The trick of adding sum() before calling backward differs with the previous application only in the order of operations performed: the summation is performed before differentiation. . From a scalar, univariate function $y=f(x)$, construct a new scalar, multivariate function $$G(x_1, dots, x_N) = f(x_1) + dots + f(x_N)$$ Using the rules of vector calculus, the gradient of $G$ at an $n$-dimensional point $(x_1, dots, x_N)$ is $$ begin{align} nabla G(x) &amp; = begin{bmatrix} frac{ partial G}{ partial x_1}(x)&amp; cdots&amp; frac{ partial G}{ partial x_N} end{bmatrix} &amp; = begin{bmatrix}f&#39;(x_1)&amp; cdots&amp;f&#39;(x_N) end{bmatrix} end{align} $$ The interpretation here is that the resulting row vector contains the gradient of $G$ at the $N$-dimensional point $(x_1, dots, x_N)$. . Computing gradients with PyTorch . A mathematical function is a mapping, which strictly speaking one should denote $f$. The denotation $y=f(x)$ is simply to suggest that the typical input will be denoted $x$ and the corresponding output will be denoted $y$. Otherwise, $y=f(x)$ actually asserts the identity between a value $y$ and the evaluation of the function $f$ at the value $x$. . In PyTorch, the primary objects are tensors, which can represent (mathematical) scalars, vectors, and matrices (as well as mathematical tensors). The way a PyTorch function calculates a tensor, generically denoted y and called the output, from another tensor, generically denoted x and called the input, reflects the action of a mathematical function $f$ (or $y=f(x)$). . Conversely, a mathematical function $f$ can be evaluated at $x$ using PyTorch, and furthermore PyTorch allows to evaluate the derivative or gradient of $f$ at $x$ via the method backward. More specifically, the backward function performs vector-Jacobian products, where the vector correspond to the gradient argument. The key point in using the backward is thus to understand how to choose the gradient argument. . The mathematical preliminaries above show how gradient should be chosen. There are two key points: . gradient has the same shape as y; | gradient is populated with 0.&#39;s and 1.&#39;s, and the location of the 1.&#39;s corresponding to the inputs and outputs of interest. | Examples revisited . . Note: The variable v is passed to the gradient argument in all our examples. . For the derivative of a scalar, univariate function evaluated a single point, we choose gradient=torch.tensor(1.), which is the default value: . import torch x = torch.tensor(1., requires_grad=True) y = x**2 v = torch.ones_like(y) y.backward() print(f&quot;Shape of x : {tuple(x.shape)}&quot;) print(f&quot;Shape of y : {tuple(y.shape)}&quot;) print(f&quot;gradient argument : {v}&quot;) . Shape of x : () Shape of y : () gradient argument : 1.0 . Note that if x is cast as a 1-dimensional tensor, then (in this particular example) y is also a 1-dimensional tensor: . import torch x = torch.tensor([1.], requires_grad=True) y = x**2 v = torch.ones_like(y) y.backward() print(f&quot;Shape of x : {tuple(x.shape)}&quot;) print(f&quot;Shape of y : {tuple(y.shape)}&quot;) print(f&quot;gradient argument : {v}&quot;) . Shape of x : (1,) Shape of y : (1,) gradient argument : tensor([1.]) . Similarly if x is cast as 2-dimensional tensor: . import torch x = torch.tensor([[1.]], requires_grad=True) y = x**2 v = torch.ones_like(y) y.backward() print(f&quot;Shape of x : {tuple(x.shape)}&quot;) print(f&quot;Shape of y : {tuple(y.shape)}&quot;) print(f&quot;gradient argument : {v}&quot;) . Shape of x : (1, 1) Shape of y : (1, 1) gradient argument : tensor([[1.]]) . For the derivative of a scalar, univariate function evaluated at multiple points, gradient contains all 1.&#39;s and is of same shape as y: . import torch x = torch.linspace(-1, 1, 5, requires_grad=True) y = x**2 v = torch.ones_like(y) y.backward(v) print(f&quot;Shape of x : {tuple(x.shape)}&quot;) print(f&quot;Shape of y : {tuple(y.shape)}&quot;) print(f&quot;gradient argument : {v}&quot;) . Shape of x : (5,) Shape of y : (5,) gradient argument : tensor([1., 1., 1., 1., 1.]) . Casting x in a different shape changes the shape of y, and thus of gradient: . import torch x = torch.linspace(-2, 2, 5).view(-1,1).requires_grad_(True) y = x**2 v = torch.ones_like(y) y.backward(v) print(f&quot;Shape of x : {tuple(x.shape)}&quot;) print(f&quot;Shape of y : {tuple(y.shape)}&quot;) print(f&quot;gradient argument : &quot;) print(v) . Shape of x : (5, 1) Shape of y : (5, 1) gradient argument : tensor([[1.], [1.], [1.], [1.], [1.]]) . For the derivative of a vector-valued, univariate function evaluated at a single point, the derivative of each component function is calculated one at a time, and gradient consists of all 0.&#39;s except for one 1., which is located at a position corresponding to the component function. In the example below, the function is in fact matrix-valued, namely we calculate the derivative of $$f(x) = begin{bmatrix}1&amp;x x^2&amp;x^3 x^4&amp;x^5 end{bmatrix} qquad textrm{at} quad x ,= ,1 ,.$$ . # collapse-show import torch import itertools x = torch.tensor(1., requires_grad=True) y = torch.stack([x**i for i in range(6)]).view(3,2) ranges = [range(_) for _ in y.shape] print(&quot;x:&quot;) print(x.data) print(&quot; ny:&quot;) print(y.data) derivatives = torch.zeros_like(y) for i, j in itertools.product(*ranges): v = torch.zeros_like(y) v[i,j] = 1. if x.grad is not None: x.grad.zero_() y.backward(v, retain_graph=True) derivatives[i,j] = x.grad.item() print(&quot; nDerivatives:&quot;) print(derivatives) . . x: tensor(1.) y: tensor([[1., 1.], [1., 1.], [1., 1.]]) Derivatives: tensor([[0., 1.], [2., 3.], [4., 5.]]) . . Note: The use of for loops can be avoided. . For the gradient of a scalar, multivariate function evaluated at a single point, gradient=torch.tensor(1.): . import torch x = torch.tensor([-1., 2.], requires_grad=True) w = torch.tensor([3., 5.]) y = (x*x*w).sum() v = torch.ones_like(y) y.backward() print(f&quot;Shape of x : {tuple(x.shape)}&quot;) print(f&quot;Shape of y : {tuple(y.shape)}&quot;) print(f&quot;gradient argument : {v}&quot;) . Shape of x : (2,) Shape of y : () gradient argument : 1.0 . In the following example, the input x is a (3,2)-tensor: . x = torch.arange(6, dtype=float).view(3,2).requires_grad_(True) y = (x**2).sum() v = torch.ones_like(y) y.backward(v) print(f&quot;Shape of x: {tuple(x.shape)}&quot;) print(f&quot;Shape of y: {tuple(y.shape)}&quot;) print(f&quot;gradient argument: {v}&quot;) print(&quot;x:&quot;) print(x.data) print(&quot;x.grad:&quot;) print(x.grad.data) . Shape of x: (3, 2) Shape of y: () gradient argument: 1.0 x: tensor([[0., 1.], [2., 3.], [4., 5.]], dtype=torch.float64) x.grad: tensor([[ 0., 2.], [ 4., 6.], [ 8., 10.]], dtype=torch.float64) .",
            "url": "https://antoinechoffrut.github.io/fastai-companion/mathematics/2020/04/19/pytorch-backward-function-vector-Jacobian-product.html",
            "relUrl": "/mathematics/2020/04/19/pytorch-backward-function-vector-Jacobian-product.html",
            "date": " • Apr 19, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Scalars, vectors, matrices, and tensors",
            "content": ". tags: scalars vectors matrices tensors mathematics linear algebra pytorch . . tl;dr . We establish the following correspondence between entities commonly encountered in linear algebra and their counterparts in PyTorch. . mathematical name mathematical notation tensor shape tensor dimension . scalar | $x$ | () | 0 | . vector | $(x_1, dots, x_n)$ | (n,) | 1 | . linear transform | $w_{i,j}$ $i=1, dots, m$$j=1, dots, n$ | N/A | N/A | . matrix | $ begin{bmatrix}w_{1,1}&amp; dots&amp;w_{1,n} vdots&amp; ddots&amp; vdots w_{m,1}&amp; dots&amp;w_{n,m} end{bmatrix}$ | (m,n) | 2 | . column vector | $ begin{bmatrix}x_1 vdots x_n end{bmatrix}$ | (n,1) | 2 | . row vector | $ begin{bmatrix}x_1&amp; dots&amp;x_n end{bmatrix}$ | (1,n) | 2 | . . Note: When we write tensor, we mean a PyTorch tensor. Otherwise, &quot;tensor&quot; refers to the mathematical notion. . . Important: The dimension of a vector is the number of its entries. The dimension of a tensor is the number of indices needed to label its entries, in other words the length of its shape. . . Note: As a consequence, it is enough to refer to a (m,n)-tensor rather than to a 2-dimensional tensor with shape (m,n), but at times we will want to emphasize the dimension and make the information explicit. . Scalars . A scalar is a real number. It corresponds to a 0-dimensional tensor with empty shape: . from torch import tensor x = tensor(3.14) print(&quot;Example of a 0-dimensional tensor in PyTorch:&quot;) print(&quot;-&quot;*45) print(x) print(f&quot; nDimension: {x.ndim}&quot;) print(f&quot;Shape: {tuple(x.shape)}&quot;) . Example of a 0-dimensional tensor in PyTorch: tensor(3.1400) Dimension: 0 Shape: () . Vectors . An $n$-dimensional vector is a list of $n$ scalars: begin{equation}(x_1, dots, x_n) ,. end{equation} It corresponds to a 0-dimensional tensor of shape (n,). . # collapse-show from torch import tensor x = tensor([-1., 5., 7.]) print(&quot;Example of a 1-dimensional tensor in PyTorch:&quot;) print(&quot;-&quot;*45) print(x) print(f&quot; nDimension: {x.ndim}&quot;) print(f&quot;Shape: {tuple(x.shape)}&quot;) . . Example of a 1-dimensional tensor in PyTorch: tensor([-1., 5., 7.]) Dimension: 1 Shape: (3,) . Linear transformations . A linear transformation which maps $n$-dimensional vectors to $m$-dimensional vectors can be identified with a doubly-indexed list of scalars $w_{i,j}$ where $i$ runs from $1$ to $m$ and $j$ from $1$ to $n$, in that they act on input vectors according to the familiar formula: $$y_i = sum_{j=1}^nw_{i,j}x_j quad textrm{for} quad i=1, dots, m ,.$$ . Tip: The indices in $w_{i,j}x_j$ appear in the order $(i, j, j)$. The dummy index $j$ is repeated next to itself and &quot;disappears&quot; upon summation, while the index $i$ remains. . Column vectors . For computational purposes, instead of a vector $(x_1, dots, x_n)$, it is convenient to work with its column vector counterpart, which is the array $$ begin{bmatrix}x_1 vdots x_n end{bmatrix} ,. $$ . Important: It is important to distinguish a vector and its corresponding column vector: $$(x_1, dots, x_n) quad ne quad begin{bmatrix}x_1 vdots x_n end{bmatrix} ,.$$ . Indeed, while the vector corresponds to a 1-dimensional tensor, its column vector counterpart corresponds to a 2-dimensional tensor with shape (n,1): . # collapse-show from torch import tensor x = tensor([-1., 5., 7.]).reshape(-1, 1) print(&quot;Example of a 1-dimensional tensor in PyTorch:&quot;) print(&quot;-&quot;*45) print(x) print(f&quot; nDimension: {x.ndim}&quot;) print(f&quot;Shape: {tuple(x.shape)}&quot;) . . Example of a 1-dimensional tensor in PyTorch: tensor([[-1.], [ 5.], [ 7.]]) Dimension: 2 Shape: (3, 1) . Matrices . An $m$-by-$n$ matrix is a doubly-indexed list $w_{i,j}$ where $i$ ranges from $1$ to $m$ and $j$ from $1$ to $n$. It is usually written as an array with $m$ rows and $n$ columns: $$ begin{bmatrix} w_{1,1}&amp; dots&amp;w_{1,j}&amp; dots&amp;w_{1,n} vdots&amp;&amp; vdots&amp;&amp; vdots w_{i,1}&amp; dots&amp;w_{i,j}&amp; dots&amp;w_{i,n} vdots&amp;&amp; vdots&amp;&amp; vdots w_{m,1}&amp; dots&amp;w_{m,j}&amp; dots&amp;w_{m,n} end{bmatrix} $$ . . Important: The first index in $w_{i,j}$ labels rows and the second index labels columns. . An $m$-by-$n$ matrix corresponds to a 2-dimensionaltensor with shape (m,n): . # collapse-show import torch n = 3 m = 4 x = torch.randn(n, m) print(&quot;Example of a 2-dimensional tensor in PyTorch:&quot;) print(&quot;-&quot;*45) print(x) print(f&quot; nDimension: {x.ndim}&quot;) print(f&quot;Shape: {tuple(x.shape)}&quot;) . . Example of a 2-dimensional tensor in PyTorch: tensor([[-0.3536, 1.9439, -0.0042, -0.8287], [-0.4937, 0.4641, -1.0579, 0.2006], [-0.4996, -1.2149, -0.9282, 0.1443]]) Dimension: 2 Shape: (3, 4) . Computation and compact representation of linear transformations . The collection of $m$ formulas $y_i = sum_{j=1}^nw_{i,j}x_j$, where $i$ runs from $1$ to $m$, is compactly represented as $$ begin{bmatrix}y_1 vdots y_m end{bmatrix} ,= , begin{bmatrix}w_{1,1}&amp; dots&amp;w_{1,n} vdots&amp; ddots&amp; vdots w_{m,1}&amp; dots&amp;w_{m,n} end{bmatrix} begin{bmatrix}x_1 vdots x_n end{bmatrix} $$ where the matrix is written to the left of the input column vector. . Note: The number of columns in the matrix matches the number of rows in the column vector. . Row vectors . Identifying a vector with its column vector is the more common convention. Alternatively, the vector $(x_1, dots, x_n)$ can be identified with its row vector counterpart, which is the array $$ begin{bmatrix}x_1&amp; dots&amp;x_n end{bmatrix} ,.$$ . Important: Again, a vector, its column and row vector counterparts are all different objects: $$(x_1, dots, x_n) quad ne quad begin{bmatrix}x_1 vdots x_n end{bmatrix} quad ne quad begin{bmatrix}x_1&amp; dots&amp;x_n end{bmatrix} quad ne quad (x_1, dots, x_n) ,.$$ . Indeed, a row vector of length $n$ corresponds to a 2-dimensional tensor with shape (1,n): . # collapse-show from torch import tensor x = tensor([-1., 5., 7.]).reshape(1, -1) print(&quot;Example of a 1-dimensional tensor in PyTorch:&quot;) print(&quot;-&quot;*45) print(x) print(f&quot; nDimension: {x.ndim}&quot;) print(f&quot;Shape: {tuple(x.shape)}&quot;) . . Example of a 1-dimensional tensor in PyTorch: tensor([[-1., 5., 7.]]) Dimension: 2 Shape: (1, 3) . . Note: Vectors and row vectors are all represented horizontally. However, vectors are written with parentheses while row vectors are written with square brackets. . Computing with row vectors . In terms of row vectors, the output of a linear transformation is more conveniently rewritten with the similar but different formulas $$y_j = sum_{i=1}^nx_i tilde w_{i,j} qquad textrm{for} quad j=1, dots, m$$ where now the matrix with coefficients $ tilde w_{i,j}$ where $i$ runs from $1$ to $n$ and $j$ from $1$ to $m$, has $n$ rows and $m$ columns: $$ begin{bmatrix} tilde w_{1,1}&amp; dots&amp; tilde w_{1,j}&amp; dots&amp; tilde w_{1,m} vdots&amp;&amp; vdots&amp;&amp; vdots tilde w_{i,1}&amp; dots&amp; tilde w_{i,j}&amp; dots&amp; tilde w_{i,m} vdots&amp;&amp; vdots&amp;&amp; vdots tilde w_{n,1}&amp; dots&amp; tilde w_{n,i}&amp; dots&amp; tilde w_{n,m} end{bmatrix} ,.$$ . . Tip: The indices in $x_i tilde w_{i,j}$ appear in the order $(i, i, j)$ (note the difference from earlier). The dummy index, which is now $i$, is repeated next to itself. The remaining index, which is now $j$, is the same as that in the quantity $y_j$ on the left of the equation. . The collection of $m$ formulas $y_j = sum_{i=1}^nx_i tilde w_{i,j}$, for $j$ from $1$ to $m$, is compactly represented as $$ begin{bmatrix}y_1&amp; dots&amp;y_m end{bmatrix} quad= quad begin{bmatrix}x_1&amp; dots&amp;x_n end{bmatrix} begin{bmatrix} tilde w_{1,1}&amp; dots&amp; tilde w_{1,m} vdots&amp; ddots&amp; vdots tilde w_{n,1}&amp; dots&amp; tilde w_{n,m} end{bmatrix} $$ where the matrix is written to the right of the input row vector. . . Note: The number of columns in the input row vector matches the number of rows in the matrix. . . Important: The coefficients $ tilde w_{i,j}$ share the same values as the coefficients $w_{i,j}$ in the representation with column vectors, but they are coefficients of distinct matrices: the first is $n$-by-$m$ while the second is $m$-by-$n$. In fact, these matrices are transposes of each other: $$ begin{bmatrix} tilde w_{1,1}&amp; dots&amp; tilde w_{1,m} vdots&amp; ddots&amp; vdots tilde w_{n,1}&amp; dots&amp; tilde w_{n,m} end{bmatrix} quad= quad begin{bmatrix}w_{1,1}&amp; dots&amp;w_{1,n} vdots&amp; ddots&amp; vdots w_{m,1}&amp; dots&amp;w_{m,n} end{bmatrix}^ top quad= quad begin{bmatrix}w_{1,1}&amp; dots&amp;w_{m,1} vdots&amp; ddots&amp; vdots w_{1,n}&amp; dots&amp;w_{m,n} end{bmatrix} ,.$$ Equivalently, $$ tilde w_{i,j} = w_{j,i} qquad textrm{for} quad i=1, dots, n ,, quad j=1, dots, m ,.$$ .",
            "url": "https://antoinechoffrut.github.io/fastai-companion/mathematics/2020/04/15/scalars-vectors-matrices-tensors.html",
            "relUrl": "/mathematics/2020/04/15/scalars-vectors-matrices-tensors.html",
            "date": " • Apr 15, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Slimfastai: mimimal imports in fastai",
            "content": "The Zen of Python and import * invocations . The fastai library is designed to safely invoke import * to import everything from a module. While this is often considered to go against Python&#39;s &quot;Explicit is better than implicit&quot;, this deliberate choice is explained in the fastai documentation. . It remains an instructive exercise to uncover which module contains a specific definition. Also, for debugging purposes, it may be useful at times to load only what is needed, to narrow down the range of possible sources of errors. . Monkey patching . In another notebook, we have seen that the fastai library adds convenience methods to classes from third party libraries. The use case was the method ls added to the pathlib class Path. This was done automatically via an import * invocation, which loads everything in the namespace in the specified libraries. . This notebook shows how to load the class Path and its added methods only. . . tags: programming exploration debugging python fastai . What is Path? . An expedient way to find out everything we need to know about Path is given from IPython&#39;s ? command: . from fastai2.vision.all import * Path? . The command above displays its output in a separate banner at the bottom of the page of the web browser. To access the same information directly: . from fastai2.vision.all import * import inspect print(&quot;Description for Path:&quot;) print(Path.__doc__) print(f&quot;Name of module defining Path: {inspect.getmodule(Path).__name__}&quot;) # or directly: Path.__module__ print(f&quot;Path to file with module source code: {inspect.getmodule(Path).__file__}&quot;) . Description for Path: PurePath subclass that can make system calls. Path represents a filesystem path but unlike PurePath, also offers methods to do system calls on path objects. Depending on your system, instantiating a Path will return either a PosixPath or a WindowsPath object. You can also instantiate a PosixPath or WindowsPath directly, but cannot instantiate a WindowsPath on a POSIX system or vice versa. Name of module defining Path: pathlib Path to file with module source code: /Users/antoine/.pyenv/versions/3.8.1/lib/python3.8/pathlib.py . Conclusion: Path is a class from the pathlib library, which is part of the Python install. . What is Path.ls? . The relevant information is displayed in a separate banner with: . from fastai2.vision.all import * Path.ls? . Direct access: . from fastai2.vision.all import * import inspect print(&quot;Description for Path.ls:&quot;) print((Path.ls).__doc__) print(f&quot;Name of module defining Path.ls: {inspect.getmodule(Path.ls).__name__}&quot;) # or directly: (Path.ls).__module__ print(f&quot;Path to file with module source code: {inspect.getmodule(Path.ls).__file__}&quot;) . Description for Path.ls: Contents of path as a list Name of module defining Path.ls: fastcore.utils Path to file with module source code: /Users/antoine/fastai/fastcore/fastcore/utils.py . Conclusion: the function ls has been added as a method to the class Path. It is defined in the fastcore library, which collects functionalities not depending on PyTorch, unlike fastai. . Minimal imports . If one wishes only to import Path and the attributes added by fastai, run . %xdel Path from pathlib import Path from fastcore.utils import ls print(f&quot;Name of module defining Path: {inspect.getmodule(Path).__name__}&quot;) # or directly: Path.__module__ print(f&quot;Path to file with module source code: {inspect.getmodule(Path).__file__}&quot;) print(f&quot;Name of module defining Path.ls: {inspect.getmodule(Path.ls).__name__}&quot;) # or directly: (Path.ls).__module__ print(f&quot;Path to file with module source code: {inspect.getmodule(Path.ls).__file__}&quot;) . Name of module defining Path: pathlib Path to file with module source code: /Users/antoine/.pyenv/versions/3.8.1/lib/python3.8/pathlib.py Name of module defining Path.ls: fastcore.utils Path to file with module source code: /Users/antoine/fastai/fastcore/fastcore/utils.py .",
            "url": "https://antoinechoffrut.github.io/fastai-companion/programming/2020/04/10/slimfastai-minimal-imports.html",
            "relUrl": "/programming/2020/04/10/slimfastai-minimal-imports.html",
            "date": " • Apr 10, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Monkey patching in fastai",
            "content": "Monkey patching . &quot;We do it in fastai all the time.&quot;1 Jeremy Howard . Monkey patching refers to the practise of dynamically modifying classes and modules. This is done quite frequently in the fastai library. . This notebook dissects one common example, namely the ls method added to the Path class from the pathlib Python library, with functionalities reminiscent of the ls command in bash. From the fastcore documentation: . We add an ls() method to pathlib.Path which is simply defined as list(Path.iterdir()), mainly for convenience in REPL environments such as notebooks. . . tags:programming python fastai exploration . The pathlib module . The pathlib module provides functionalities to handle filesystem paths. . from pathlib import Path path = Path().home() dir_native = dir(path) print(f&quot;Path to home directory: {str(path)}&quot;) print(f&#39;Object path has attribute ls: {&quot;ls&quot; in dir_native}&#39;) . Path to home directory: /Users/antoine Object path has attribute ls: False . The class Path does not have a method ls. . Supercharging Path with fastai . We first remove the Path class from our scope so that we can load it again via fastai (read on): . %xdel Path try: Path print(&quot;This will fail so this won&#39;t print&quot;) except NameError: print(&quot;Class `Path` has been removed.&quot;) . NameError: name &#39;Path&#39; is not defined Class `Path` has been removed. . The following will automatically load Path (among many other things): . from fastai2.vision.all import * path = Path().home() dir_fastai = dir(path) print(f&quot;Path to home directory: {str(path)}&quot;) print(f&#39;Object path has attribute ls: {&quot;ls&quot; in dir_fastai}&#39;) . Path to home directory: /Users/antoine Object path has attribute ls: True . The ls method is one of several attributes that fastai (in fact, fastcore, read on) adds to the Path class. The list of added attributes is: . set(dir_fastai) - set(dir_native) . {&#39;load&#39;, &#39;load_array&#39;, &#39;ls&#39;, &#39;read&#39;, &#39;readlines&#39;, &#39;save&#39;, &#39;save_array&#39;, &#39;write&#39;} . See the fastcore.utils documentation for other File and network functions. . Loading Path and Path.ls only? . It is an instructive exercise to find out how to load Path and its attributes added in the fastai library only. This is done in another notebook. . Footnotes . 1. Swift For TensorFlow AND FastAI: Part 2, featuring Chris Lattner↩ .",
            "url": "https://antoinechoffrut.github.io/fastai-companion/programming/2020/04/08/monkey-patching-in-fastai.html",
            "relUrl": "/programming/2020/04/08/monkey-patching-in-fastai.html",
            "date": " • Apr 8, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "Know your system",
            "content": "This notebook . This notebook shows how to find information about your system such as: . OS information | python version and install location | package and module versions and install locations | . My system . Some adjusments may be needed if you work on different platforms (especially Windows), or if you use other managers, such as conda: . I work locally (mostly) on a mac; | I use pyenv as python version manager, and its plug-in pyenv-virtualenv as virtual environment manager. | . . tags: system platform virtual environment management install python jupyter pytorch fastai workflow productivity . Tools and references . Python modules . sys | site | platform | . Utilities . pip: Python package management system | pyenv: Python version management system | pyenv-virtualenv: virtual environment management system | jupyter: interactive platform for scientific computing | . Helpful commands . Built-in magic commands (IPython documentation) | Python vs IPython (IPython documentation) | . Useful references . 3 ways to pip install a package - fastai2 use case by Farid Hussainia | How do I find the location of my python site packages directory? (stackoverflow) | . . Environment . System . I usually work locally on a MacBook Pro (2018). In particular I don&#39;t have a GPU (at least not a dedicated GPU). . # collapse-hide import platform print(f&quot;Platform: {platform.platform()} n&quot;) print(&quot;Details:&quot;) print(f&quot;Machine type: {platform.machine()}&quot;) print(f&quot;System: {platform.system()}&quot;) print(f&quot;Version: {platform.version()}&quot;) print(f&quot;Processor: {platform.processor()}&quot;) print(f&quot;Release: {platform.release()}&quot;) print(f&quot;Mac OS version: {platform.mac_ver()}&quot;) . . Platform: macOS-10.15.4-x86_64-i386-64bit Details: Machine type: x86_64 System: Darwin Version: Darwin Kernel Version 19.4.0: Wed Mar 4 22:28:40 PST 2020; root:xnu-6153.101.6~15/RELEASE_X86_64 Processor: i386 Release: 19.4.0 Mac OS version: (&#39;10.15.4&#39;, (&#39;&#39;, &#39;&#39;, &#39;&#39;), &#39;x86_64&#39;) . Virtual environments (with pyenv) . I use pyenv to manage Python versions, and the extension pyenv-virtualenv to manage virtual environments. . !pyenv -v . pyenv 1.2.17 . To check which virtual environment is activated (with pyenv): . !pyenv version . fastai2 (set by PYENV_VERSION environment variable) . Python version . Using the sys module . import sys print(sys.version) # alternatively: print(sys.version_info) . 3.8.1 (default, Mar 30 2020, 15:12:05) [Clang 11.0.3 (clang-1103.0.32.29)] . Using the platform module . import platform print(f&quot;Python: version {platform.python_version()}&quot;) print(f&quot;Python build: {platform.python_build()}&quot;) . Python: version 3.8.1 Python build: (&#39;default&#39;, &#39;Mar 30 2020 15:12:05&#39;) . A convoluted alternative . !python --version . Python 3.8.1 . Installed packages . List of all packlages with pip . # !pip freeze # uncomment to see full list . Note that those lines beginning with -e correspond to packages installed in editable mode. For example, following Farid Hassainia&#39;s post, I have an editable install of fastai2 (see below). . Check specific packages . Check that fastai2 is installed . !pip freeze | grep fastai2 . -e git+https://github.com/fastai/fastai2@44491962b1d79b2db0e89ffb3eee5129e9e1d489#egg=fastai2 . This is an editable install. Otherwise, packages installed via pip are listed as usual. Check that (e.g.). numpy is installed: . !pip freeze | grep numpy . numpy==1.18.2 . Package versions . Using the __version__ attribute . import torch print(f&quot;{torch.__name__:&lt;12}: version {torch.__version__}&quot;) import fastai2 print(f&quot;{fastai2.__name__:&lt;12}: version {fastai2.__version__}&quot;) . torch : version 1.4.0 fastai2 : version 0.0.17 . Locating the Python install (and chasing symlinks) . Using sys . Returns absolute path to python executable . import sys sys.executable . &#39;/Users/antoine/.pyenv/versions/3.8.1/envs/fastai2/bin/python3.8&#39; . Actually, this is a symlink: . !ls -l /Users/antoine/.pyenv/versions/3.8.1/envs/fastai2/bin/python3.8 . lrwxr-xr-x 1 antoine staff 50 2 Apr 10:56 /Users/antoine/.pyenv/versions/3.8.1/envs/fastai2/bin/python3.8 -&gt; /Users/antoine/.pyenv/versions/3.8.1/bin/python3.8 . !ls -l /Users/antoine/.pyenv/versions/3.8.1/bin/python3.8 . -rwxr-xr-x 1 antoine staff 3572068 30 Mar 15:12 /Users/antoine/.pyenv/versions/3.8.1/bin/python3.8 . This is the executable: . !file /Users/antoine/.pyenv/versions/3.8.1/bin/python3.8 . /Users/antoine/.pyenv/versions/3.8.1/bin/python3.8: Mach-O 64-bit executable x86_64 . Using bash command which . Remark: pyenv uses shims, and running the comand which python in the cell below returns a different output if run from a terminal. In that case, run pyenv which python. . !which python . /Users/antoine/.pyenv/versions/fastai2/bin/python . This is a symlink: . !ls -l /Users/antoine/.pyenv/versions/fastai2/bin/python . lrwxr-xr-x 1 antoine staff 9 2 Apr 10:56 /Users/antoine/.pyenv/versions/fastai2/bin/python -&gt; python3.8 . What does it point to? . !type python3.8 . python3.8 is /Users/antoine/.pyenv/versions/fastai2/bin/python3.8 . This is again a symlink: . !ls -l /Users/antoine/.pyenv/versions/fastai2/bin/python3.8 . lrwxr-xr-x 1 antoine staff 50 2 Apr 10:56 /Users/antoine/.pyenv/versions/fastai2/bin/python3.8 -&gt; /Users/antoine/.pyenv/versions/3.8.1/bin/python3.8 . which we have seen above. . Locating packages . Read the __path__ attribute . For a package installe via pip in the usual way: . import numpy numpy.__path__ . [&#39;/Users/antoine/.pyenv/versions/3.8.1/envs/fastai2/lib/python3.8/site-packages/numpy&#39;] . For an editable install, from a local copy of the repository: . import fastai2 fastai2.__path__ . [&#39;/Users/antoine/fastai/fastai2/fastai2&#39;] . Using !pip show . !pip show numpy | grep Location # !pip show numpy # uncomment to see more info on package . Location: /Users/antoine/.pyenv/versions/3.8.1/envs/fastai2/lib/python3.8/site-packages . For an editable install: . !pip show fastai2 | grep Location #!pip show fastai2 # uncomment to see more info on package . Location: /Users/antoine/fastai/fastai2 . Search path for modules: sys.path . Python looks for paths to modules by looking into a list of directories in sys.path: . import sys sys.path . [&#39;/Users/antoine/fastai/fastai-companion/_notebooks&#39;, &#39;/Users/antoine/.pyenv/versions/3.8.1/lib/python38.zip&#39;, &#39;/Users/antoine/.pyenv/versions/3.8.1/lib/python3.8&#39;, &#39;/Users/antoine/.pyenv/versions/3.8.1/lib/python3.8/lib-dynload&#39;, &#39;&#39;, &#39;/Users/antoine/.pyenv/versions/3.8.1/envs/fastai2/lib/python3.8/site-packages&#39;, &#39;/Users/antoine/fastai/fastai2&#39;, &#39;/Users/antoine/fastai/fastcore&#39;, &#39;/Users/antoine/fastai/nbdev&#39;, &#39;/Users/antoine/.pyenv/versions/3.8.1/envs/fastai2/lib/python3.8/site-packages/IPython/extensions&#39;, &#39;/Users/antoine/.ipython&#39;] . List of all global site-packages directories . Packages are normally installed in the site-packages folder of the Python installation. This directory can be found using the site module: . import site site.getsitepackages() . [&#39;/Users/antoine/.pyenv/versions/3.8.1/envs/fastai2/lib/python3.8/site-packages&#39;] .",
            "url": "https://antoinechoffrut.github.io/fastai-companion/system/2020/03/31/know-your-system.html",
            "relUrl": "/system/2020/03/31/know-your-system.html",
            "date": " • Mar 31, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About",
          "content": "I am a mathematician by training, with about a decade of research experience in academia, where I specialized in the analysis of partial differential equations. . Recently my interests turned to data science, particularly neural networks. .",
          "url": "https://antoinechoffrut.github.io/fastai-companion/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

}