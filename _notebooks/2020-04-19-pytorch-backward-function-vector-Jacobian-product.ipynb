{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing gradients and derivatives in PyTorch\n",
    ">\"Making good use of the `gradient` argument in PyTorch's `backward` function\"\n",
    "\n",
    "- toc: true \n",
    "- badges: true\n",
    "- comments: true\n",
    "- categories: [mathematics]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "tags: mathematics pytorch gradients backward automatic differentiation vector-Jacobian product backpropagation\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tl;dr\n",
    "The `backward` function in `PyTorch` can be used to compute the derivatives or gradients of functions.  The `backward` function computes vector-Jacobian products so that the appropriate vector must be determined.  In other words, the correct `gradient` argument must be passed to `backward`, although not passing `gradient` explicitly will cause `backward` to choose the appropriate value but only in the simplest cases.\n",
    "\n",
    "This notebook explains vector-Jacobian products and how to choose the `gradient` argument in the `backward` function in the general case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A brief overview\n",
    "In the case of a function taking a scalar and returning a scalar, the use of the `backward` function is quite straight-forward:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Derivative at a single point:\n",
      "tensor(2.)\n"
     ]
    }
   ],
   "source": [
    "# collapse-hide\n",
    "import torch\n",
    "x = torch.tensor(1., requires_grad=True)\n",
    "y = x**2\n",
    "y.backward()\n",
    "print(f\"Derivative at a single point:\")\n",
    "print(x.grad.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, when  \n",
    "- the function is **multi-valued** (e.g. vector- or matrix-valued); or  \n",
    "- one wishes to compute the derivative of a function at **mulitple** points,  \n",
    "\n",
    "then the `gradient` argument in `backward` must be suitably chosen.  For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Derivative at multiple points:\n",
      "tensor([-4., -2.,  0.,  2.,  4.])\n"
     ]
    }
   ],
   "source": [
    "# collapse-hide\n",
    "import torch\n",
    "x = torch.linspace(-2, 2, 5, requires_grad=True)\n",
    "y = x**2\n",
    "gradient = torch.ones_like(y)\n",
    "y.backward(gradient)\n",
    "print(\"Derivative at multiple points:\")\n",
    "print(x.grad.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed, more precisely, the `backward` function computes vector-Jacobian products, which is not explicit in the function's doc string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First line of `torch.Tensor.backward` doc string:\n",
      "\"Computes the gradient of current tensor w.r.t. graph leaves.\"\n"
     ]
    }
   ],
   "source": [
    "# collapse-hide\n",
    "print(\"First line of `torch.Tensor.backward` doc string:\")\n",
    "print(\"\\\"\"+ torch.Tensor.backward.__doc__.split(\"\\n\")[0] + \"\\\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "although some explanations are given [in this official tutorial](https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html#gradients).  The crucial point is therefore to choose the appropriate vector, which is passed to the `backward` function in its `gradient` argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Tensor.backward(self, gradient=None, retain_graph=None, create_graph=False)\n",
      "...\n",
      "        Arguments:\n",
      "            gradient (Tensor or None): Gradient w.r.t. the\n",
      "                tensor. If it is a tensor, it will be automatically converted\n",
      "                to a Tensor that does not require grad unless ``create_graph`` is True.\n",
      "                None values can be specified for scalar Tensors or ones that\n",
      "                don't require grad. If a None value would be acceptable then\n",
      "                this argument is optional.\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "# collapse-hide\n",
    "import inspect\n",
    "import torch\n",
    "print(f\"torch.Tensor.backward{inspect.signature(torch.Tensor.backward)}\")\n",
    "print(\"...\")\n",
    "print(\"\\n\".join(torch.Tensor.backward.__doc__.split(\"\\n\")[11:18]))\n",
    "print(\"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a way around specifying the `gradient` argument.  Revisiting the example above, the derivative at multiple points can be equivalently calculated by adding a `sum()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Derivative at multiple points:\n",
      "tensor([-4., -2.,  0.,  2.,  4.])\n"
     ]
    }
   ],
   "source": [
    "# collapse-hide\n",
    "import torch\n",
    "x = torch.linspace(-2, 2, 5, requires_grad=True)\n",
    "y = (x**2).sum()\n",
    "\n",
    "y.backward()\n",
    "print(\"Derivative at multiple points:\")\n",
    "print(x.grad.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the `backward` method is invoked on a different `tensor`:  \n",
    "```\n",
    "(x**2).backward()\n",
    "```\n",
    "if `x` contains a single input,\n",
    "vs\n",
    "```\n",
    "(x**2).sum().backward()\n",
    "```\n",
    "if `x` contains multiple inputs.\n",
    "\n",
    "On the other hand, passing the `gradient` argument, whether `x` contains one or multiple inputs, the same command is used to compute the derivatives:\n",
    "```\n",
    "y = (x**2)\n",
    "y.backward(torch.ones_like(y))\n",
    "```\n",
    "\n",
    "Roughly speaking, the difference between the two methods, namely setting `gradient=torch.ones_like(y)` or adding `sum()`, is in the order of the summation and differentiation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Usage examples of the `backward` function\n",
    "The derivative of the **scalar**, **univariate** function $f(x)=x^2$ at a **single** point $x=1$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.tensor(1., requires_grad=True)\n",
    "y = x**2\n",
    "y.backward()\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The derivative of the **scalar**, **univariate** function $f(x)=x^2$ at **multiple** points $x= -2, -1, \\dots, 2$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-4., -2.,  0.,  2.,  4.])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.linspace(-2, 2, 5, requires_grad=True)\n",
    "y = x**2\n",
    "v = torch.ones_like(y)\n",
    "y.backward(v)\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradient of the **scalar**, **multivariate** function $f(x_1, x_2)=3x_1^2 + 5x_2^2$ at a **single** point $(x_1, x_2)=(-1, 2)$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-6., 20.])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.tensor([-1., 2.], requires_grad=True)\n",
    "w = torch.tensor([3., 5.])\n",
    "y = (x*x*w).sum()\n",
    "y.backward()\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradient of the **scalar**, **multivariate** function $f(x_1, x_2) = -x_1^2 + x_2^2$ at **multiple** points $(x_1, x_2)$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.,  2.],\n",
       "        [-4.,  6.],\n",
       "        [-8., 10.]], dtype=torch.float64)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.arange(6, dtype=float).view(3, 2).requires_grad_(True)\n",
    "w = torch.tensor([-1, 1])\n",
    "y = (x*x*w).sum(1)\n",
    "v = torch.ones_like(y)\n",
    "y.backward(v)\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The _derivatives_ of the **vector-valued**, **univariate** function $f(x)= (-x^3, 5x)$ at a **single** point $x=1$, i.e. the derivative of\n",
    "- its first component function $f_1(x)=-x^3$; and\n",
    "- its second component function $f_2(x)=5x$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f_1'(1.0) = -3.0\n",
      "f_2'(1.0) =  5.0\n"
     ]
    }
   ],
   "source": [
    "# collapse-hide\n",
    "import torch\n",
    "x = torch.tensor(1., requires_grad=True)\n",
    "y = torch.stack([-x**3, 5*x])\n",
    "\n",
    "v1 = torch.tensor([1., 0.])\n",
    "y.backward(v1, retain_graph=True)\n",
    "\n",
    "print(f\"f_1'({x.data.item()}) = {x.grad.data.item():>4}\")\n",
    "\n",
    "x.grad.zero_()\n",
    "\n",
    "v2 = torch.tensor([0., 1.])\n",
    "y.backward(v2)\n",
    "print(f\"f_2'({x.data.item()}) = {x.grad.data.item():>4}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The _derivatives_ of the **vector-valued**, **univariate** function $f(x)= (-x^3, 5x)$ at **multiple** points, i.e. the derivative of\n",
    "- its first component function $f_1(x)=-x^3$; and\n",
    "- its second component function $f_2(x)=5x$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Derivative of f_1(x)=-3x^2 at the points (0.0, 1.0, 2.0):\n",
      "tensor([  0.,  -3., -12.], dtype=torch.float64)\n",
      "\n",
      "Derivative of f_2(x)=5x at the points (0.0, 1.0, 2.0):\n",
      "tensor([5., 5., 5.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# collapse-hide\n",
    "import torch\n",
    "import itertools\n",
    "x = torch.arange(3, dtype=float, requires_grad=True)\n",
    "y = torch.stack([-x**3, 5*x])\n",
    "\n",
    "ranges = [range(_) for _ in y.shape]\n",
    "\n",
    "v1 = torch.tensor([1. if i == 0 else 0. for i, j in itertools.product(*ranges)]).view(*y.shape)\n",
    "y.backward(v1, retain_graph=True)\n",
    "print(f\"Derivative of f_1(x)=-3x^2 at the points {tuple(x.data.view(-1).tolist())}:\")\n",
    "print(x.grad)\n",
    "\n",
    "x.grad.zero_()\n",
    "\n",
    "v2 = torch.tensor([1. if i == 1 else 0. for i, j in itertools.product(*ranges)]).view(*y.shape)\n",
    "y.backward(v2)\n",
    "print(f\"\\nDerivative of f_2(x)=5x at the points {tuple(x.data.view(-1).tolist())}:\")\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The _gradients_ of the **vector-valued**, **multivariate** function\n",
    "$$\n",
    "f(x_1, \\dots, x_n) = (x_1 + \\dots + x_n\\,, x_1^2 + \\dots + x_n^2)\n",
    "$$\n",
    "at a **single** point $(x_1, \\dots, x_n)$, i.e. the gradient of\n",
    "- its first component function $f_1(x_1, \\dots, x_n) = x_1 + \\dots + x_n$; and\n",
    "- its second component function $f_2(x_1, \\dots, x_n) = x_1^2 + \\dots + x_n^2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x                 : (0.0, 1.0, 2.0, 3.0)\n",
      "y = (y_1, y_2)    : (6.0, 14.0)\n",
      "gradient of y_1   : (1.0, 1.0, 1.0, 1.0)\n",
      "gradient of y_2   : (0.0, 2.0, 4.0, 6.0)\n"
     ]
    }
   ],
   "source": [
    "# collapse-show\n",
    "import torch\n",
    "x = torch.arange(4, dtype=float, requires_grad=True)\n",
    "y = torch.stack([x.sum(), (x**2).sum()])\n",
    "\n",
    "print(f\"x                 : {tuple(x.data.tolist())}\")\n",
    "print(f\"y = (y_1, y_2)    : {tuple(y.data.tolist())}\")\n",
    "\n",
    "v1 = torch.tensor([1., 0.])\n",
    "y.backward(v1, retain_graph=True)\n",
    "print(f\"gradient of y_1   : {tuple(x.grad.data.tolist())}\")\n",
    "\n",
    "x.grad.zero_()\n",
    "\n",
    "v2 = torch.tensor([0., 1.])\n",
    "y.backward(v2)\n",
    "print(f\"gradient of y_2   : {tuple(x.grad.data.tolist())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The _gradients_ of the **vector-valued**, **multivariate** function\n",
    "$$\n",
    "f(x_1, \\dots, x_n) = (x_1 + \\dots + x_n\\,, x_1^2 + \\dots + x_n^2)\n",
    "$$\n",
    "at **multiple** points, i.e. the gradient of\n",
    "- its first component function $f_1(x_1, \\dots, x_n) = x_1 + \\dots + x_n$; and\n",
    "- its second component function $f_2(x_1, \\dots, x_n) = x_1^2 + \\dots + x_n^2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x:\n",
      "tensor([[ 0.,  1.,  2.,  3.],\n",
      "        [ 4.,  5.,  6.,  7.],\n",
      "        [ 8.,  9., 10., 11.]], dtype=torch.float64)\n",
      "y:\n",
      "tensor([[  6.,  22.,  38.],\n",
      "        [ 14., 126., 366.]], dtype=torch.float64)\n",
      "\n",
      "Gradients of the f1 at multiple points:\n",
      "tensor([[1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.]], dtype=torch.float64)\n",
      "\n",
      "Gradients of the f2 at multiple points:\n",
      "tensor([[ 0.,  2.,  4.,  6.],\n",
      "        [ 8., 10., 12., 14.],\n",
      "        [16., 18., 20., 22.]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# collapse-show\n",
    "import torch\n",
    "import itertools\n",
    "x = torch.arange(4*3, dtype=float).view(-1,4).requires_grad_(True)\n",
    "y = torch.stack([x.sum(1), (x**2).sum(1)])\n",
    "print(\"x:\")\n",
    "print(x.data)\n",
    "print(\"y:\")\n",
    "print(y.data)\n",
    "\n",
    "print()\n",
    "\n",
    "ranges = [range(_) for _ in y.shape]\n",
    "\n",
    "v1 = torch.tensor([1. if i == 0 else 0. for i, j in itertools.product(*ranges)]).view(*y.shape)\n",
    "y.backward(v1, retain_graph=True)\n",
    "print(\"Gradients of the f1 at multiple points:\")\n",
    "print(x.grad)\n",
    "\n",
    "x.grad.zero_()\n",
    "\n",
    "print()\n",
    "v2 = torch.tensor([1. if i == 1 else 0. for i, j in itertools.product(*ranges)]).view(*y.shape)\n",
    "y.backward(v2)\n",
    "print(\"Gradients of the f2 at multiple points:\")\n",
    "print(x.grad)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mathematical preliminaries\n",
    "## Scalars, vectors, matrices, and tensors\n",
    "\n",
    "- A **scalar** is a real number.  It is usually denoted with $x$. \n",
    "- An **$n$-dimensional vector** is a list $(x_1, \\dots, x_n)$ of scalars.\n",
    "- An **$m$-by-$n$ matrix** is an array with $m$ rows and $n$ columns of scalars:\n",
    "$$\n",
    "\\begin{bmatrix}w_{1,1}&\\dots&w_{1,n}\\\\\\vdots&\\ddots&\\vdots\\\\w_{m,1}&\\dots&w_{m,n}\\end{bmatrix}\n",
    "$$\n",
    "- A **column vector** of length $n$ is a $n$-by-$1$ matrix:\n",
    "$$\\begin{bmatrix}x_1\\\\\\vdots\\\\x_n\\end{bmatrix}$$\n",
    "Note that it is distinct from its vector counterpart $(x_1, \\dots, x_n)$.\n",
    "- A **row vector** of length $n$ is a $1$-by-$n$ matrix:\n",
    "$$\\begin{bmatrix}x_1&\\dots&x_n\\end{bmatrix}$$\n",
    "Note that it is distinct from its vector and column vector counterparts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Note:\n",
    "For convenience, we may denote a vector, a column vector, or a row vector with a single symbol, typically $x$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In another post we establish the following correspondence between these mathematical entities and their `tensor` counterparts in `PyTorch`:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|mathematical name|mathematical notation|`tensor` shape|`tensor` dimension|\n",
    "|---|---|---|---|\n",
    "|scalar|$x$|`()`|`0`|\n",
    "|vector|$(x_1, \\dots, x_n)$|`(n,)`|`1`|\n",
    "|matrix|$\\begin{bmatrix}w_{1,1}&\\dots&w_{1,n}\\\\\\vdots&\\ddots&\\vdots\\\\w_{m,1}&\\dots&w_{n,m}\\end{bmatrix}$|`(m,n)`| `2`|\n",
    "|column vector|$\\begin{bmatrix}x_1\\\\\\vdots\\\\x_n\\end{bmatrix}$|`(n,1)`|`2`|\n",
    "|row vector|$\\begin{bmatrix}x_1&\\dots&x_n\\end{bmatrix}$|`(1,n)`|`2`|\n",
    "\n",
    "## Mathematical functions\n",
    "- We consider functions which are mappings from scalars, vectors, or matrices to scalars, vectors, or matrices.  It is generically denoted $y=f(x)$.\n",
    "- A **scalar** function $y=f(x)$ is a function returning a scalar, i.e. $y$ is a scalar.  \n",
    "- A **vector-valued** function $y=f(x)$ is a function returning a vector, i.e. $y$ is a vector.  We often write\n",
    "$$f(x) = (f_1(x), \\dots, f_m(x))$$\n",
    "if the output is $m$-dimensional, where each of $f_1(x), \\dots, f_m(x)$ is a scalar function.\n",
    "- A **univariate** function $y=f(x)$ is a function depending on a scalar $x$.\n",
    "- A **multivariate** function $y=f(x)$ is a function depending on a vector $x=(x_1, \\dots, x_n)$.  \n",
    "\n",
    "In summary\n",
    "\n",
    "|$y=f(x)$|scalar-valued|vector-valued|\n",
    "|---|---|---|\n",
    "|**univariate**|$x$ is a scalar<br>$y$ is a scalar|$x$ is a scalar<br>$y$ is a vector|\n",
    "|**multivariate**|$x$ is a vector<br>$y$ is a scalar|$x$is a vector<br>$y$ is a vector|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Differentiation\n",
    "### Basic definitions\n",
    "We do not recall the definitions for:\n",
    "- the **derivative** $f'(x)$ of a scalar, uni-variate function $y=f(x)$ evaluated at a scalar $x$;\n",
    "- the **partial derivatives** $\\frac{\\partial f}{\\partial x_i}(x)$, $i=1, \\dots, n$, of a scalar, multivariate function $y=f(x)$ with respect to the variables $x_1, \\dots, x_n$, and evaluated at $x=(x_1, \\dots, x_n)$.\n",
    "\n",
    "### Derivatives of vector-valued, univariate functions\n",
    "The **derivative** of a vector-valued, uni-variate function $y=f(x)$ evaluated at a scalar $x$ is the vertical concatenation of the derivatives of its component functions:\n",
    "$$f'(x) = \\begin{bmatrix}f_1'(x)\\\\\\vdots\\\\f_m'(x)\\end{bmatrix}$$\n",
    "\n",
    "### Gradients\n",
    "The **gradient** of a scalar-valued function $y=f(x)$, is the *row* vector of its partial derivatives:\n",
    "$$\\nabla f(x) = \\begin{bmatrix}\\frac{\\partial f}{\\partial x_1}(x)&\\dots&\\frac{\\partial f}{\\partial x_n}(x)\\end{bmatrix}$$\n",
    "with length $n$ if $x$ is $n$-dimensional: $x=(x_1, \\dots, x_n)$.\n",
    "\n",
    "### Jacobians\n",
    "The **Jacobian** of a vector-valued, multivariate function $y=f(x)$ is the vertical concatenation of the gradients of the component functions $f_1, \\dots, f_m$:\n",
    "$$J_f(x)\n",
    "\\,=\\,\n",
    "\\begin{bmatrix}\n",
    "\\nabla f_1(x)\\\\\\vdots\\\\\\nabla f_m(x)\n",
    "\\end{bmatrix}\n",
    "\\,=\\,\n",
    "\\begin{bmatrix}\n",
    "    \\frac{\\partial f_1}{\\partial x_1}(x)&\\dots&\\frac{\\partial f_1}{\\partial x_n}(x)\\\\\n",
    "    \\vdots&\\ddots&\\vdots\\\\\n",
    "    \\frac{\\partial f_m}{\\partial x_1}(x)&\\dots&\\frac{\\partial f_m}{\\partial x_n}(x)\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "It is thus an $m$-by-$n$ matrix, i.e. with $m$ rows and $n$ columns.\n",
    "\n",
    "#### Special case: $m=1$\n",
    "In case $m=1$, the Jacobian agrees with the gradient of a scalar, multivariate function:\n",
    "$$J_f(x) = \\nabla f(x)$$\n",
    "\n",
    "#### Special case: $n=1$\n",
    "In case $n=1$, the Jacobian agrees with the derivative of a vector-valued, univariate function.\n",
    "$$J_f(x) = \\begin{bmatrix}f_1'(x)\\\\\\vdots\\\\f_m'(x)\\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector-Jacobian products\n",
    "Given a vector-valued, multivariate function $y=f(x)$ and a _column_ vector\n",
    "$v=\\begin{bmatrix}v_1\\\\\\vdots\\\\v_m\\end{bmatrix}$,\n",
    "the **vector-Jacobian product** is the matrix multiplication\n",
    "$$v^\\top J_f(x) \\,=\\,\n",
    "\\begin{bmatrix}\n",
    "v_1&\\dots&v_m\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "    \\frac{\\partial f_1}{\\partial x_1}(x)&\\dots&\\frac{\\partial f_1}{\\partial x_n}(x)\\\\\n",
    "    \\vdots&\\ddots&\\vdots\\\\\n",
    "    \\frac{\\partial f_m}{\\partial x_1}(x)&\\dots&\\frac{\\partial f_m}{\\partial x_n}(x)\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "which is then a _row_ vector of length $n$.\n",
    "\n",
    "### Special case\n",
    "If $v^\\top$ happens to be the gradient of a scalar-valued function $z=\\ell(y)$ evaluated at $f(x)$, i.e. $v = \\nabla \\ell(y)$ where $y=f(x)$, then\n",
    "\\begin{equation}\n",
    "v^\\top J_f(x) \n",
    "\\,=\\,\\nabla (\\ell\\circ f)(x)\n",
    "\\end{equation}\n",
    "In other words, $v^\\top J_f(x)$ is the gradient of the composition of the function $\\ell$ with the function $f$.\n",
    "\n",
    ">Note:\n",
    "The vector-Jacobian product can be generalized to cases where $x$ and $y$ are (mathematical) tensors of higher dimensions.  This generalization is in fact used in some of the examples of this post."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Application: Gradients of vector-valued functions\n",
    "If $y=f(x)=(f_1(x), \\dots, f_m(x))$ is a vector-valued, multivariate function, one computes the gradients $\\nabla f_1(x), \\dots, \\nabla f_m(x)$ one at a time, each time with a suitable vector $v$.  Indeed, fix $i$ between $1$ and $m$, and define $\\ell_i(y)=y_i$ the function selecting the $i$-th coordinate of $y=(y_1, \\dots, y_m)$, so that\n",
    "$$f_i(x) = \\ell_i(f(x))\\,.$$\n",
    "Noting that\n",
    "$$\\nabla \\ell_i(y) = \\begin{bmatrix}0&\\cdots&0&1&0&\\cdots&0\\end{bmatrix}$$\n",
    "where the only non-zero coordinate is in the $i$-th position, then \n",
    "$$\n",
    "\\begin{align}\n",
    "\\nabla \\ell_i(f(x))J_f(x)\n",
    "& =\n",
    "\\begin{bmatrix}0&\\cdots&0&1&0&\\cdots&0\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "    \\frac{\\partial f_1}{\\partial x_1}(x)&\\dots&\\frac{\\partial f_1}{\\partial x_n}(x)\\\\\n",
    "    \\vdots&\\ddots&\\vdots\\\\\n",
    "    \\frac{\\partial f_m}{\\partial x_1}(x)&\\dots&\\frac{\\partial f_m}{\\partial x_n}(x)\n",
    "\\end{bmatrix}\\\\\n",
    "&=\n",
    "\\begin{bmatrix}\\frac{\\partial f_i}{\\partial x_1}(x)&\\dots&\\frac{\\partial f_i}{\\partial x_n}(x)\\end{bmatrix}\n",
    "\\end{align}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Application: Derivatives at multiple points\n",
    "To evaluate the derivative of a scalar, univariate function $f(x)$ at multiple sample points $x^{(1)}, \\dots, x^{(N)}$, we create a *new*, vector-valued and multivariate function\n",
    "$$F(x)=\\begin{bmatrix}f\\left(x^{(1)}\\right)\\\\ \\vdots \\\\ f\\left(x^{(N)}\\right)\\end{bmatrix}\n",
    "\\qquad\\textrm{where}\\qquad\n",
    "x\\,=\\,(x^{(1)}, \\dots, x^{(N)})\\,.$$\n",
    "Thus, its Jacobian is\n",
    "$$J_F(x)=\\begin{bmatrix}\n",
    "f'(x^{(1)})&&&&\\\\\n",
    "&\\ddots&&&\\\\\n",
    "&&f'(x^{(j)})&&\\\\\n",
    "&&&\\ddots&\\\\\n",
    "&&&&f'(x^{(N)})\\end{bmatrix}\n",
    "$$\n",
    "where all off-diagonal terms are $0$.\n",
    "Thus, setting $v=\\begin{bmatrix}1\\\\\\vdots\\\\1\\end{bmatrix}$, we obtain the gradient of $f$ evaluated at the $N$ sample points $x^{(1)}\\,, \\dots\\,, x^{(N)}$:\n",
    "$$\\begin{bmatrix}f'(x^{(1)})&\\dots& f'(x^{(j)})&\\cdots& f'(x^{(N)})\\end{bmatrix}\n",
    "=\\left[1\\,,\\dots\\,,1\\right]\n",
    "J_f(x)\\,.$$\n",
    "The interpretation here is that the resulting row vector contains the derivative of $f$ at the samples $x^{(1)}$ to $x^{(N)}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The trick with `sum()`\n",
    "The trick of adding `sum()` before calling `backward` differs with the previous application only in the order of operations performed: the summation is performed before differentiation.\n",
    "\n",
    "From a scalar, univariate function $y=f(x)$, construct a new scalar, multivariate function \n",
    "$$G(x_1, \\dots, x_N) = f(x_1) + \\dots + f(x_N)$$\n",
    "Using the rules of vector calculus, the gradient of $G$ at an $n$-dimensional point $(x_1, \\dots, x_N)$ is\n",
    "$$\n",
    "\\begin{align}\n",
    "\\nabla G(x) & = \\begin{bmatrix}\\frac{\\partial G}{\\partial x_1}(x)&\\cdots&\\frac{\\partial G}{\\partial x_N}\\end{bmatrix}\\\\\n",
    "& = \\begin{bmatrix}f'(x_1)&\\cdots&f'(x_N)\\end{bmatrix}\n",
    "\\end{align}\n",
    "$$\n",
    "The interpretation here is that the resulting row vector contains the gradient of $G$ at the $N$-dimensional point $(x_1, \\dots, x_N)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing gradients with `PyTorch`\n",
    "A mathematical function is a mapping, which strictly speaking one should denote $f$.  The denotation $y=f(x)$ is simply to suggest that the typical input will be denoted $x$ and the corresponding output will be denoted $y$.  Otherwise, $y=f(x)$ actually asserts the identity between a value $y$ and the evaluation of the function $f$ at the value $x$.\n",
    "\n",
    "In `PyTorch`, the primary objects are `tensor`s, which can represent (mathematical) scalars, vectors, and matrices (as well as mathematical tensors).   The way a `PyTorch` function calculates a `tensor`, generically denoted `y` and called the output, from another `tensor`, generically denoted `x` and called the input, reflects the action of a mathematical function $f$ (or $y=f(x)$).\n",
    "\n",
    "Conversely, a mathematical function $f$ can be evaluated at $x$ using `PyTorch`, and furthermore `PyTorch` allows to evaluate the derivative or gradient of $f$ at $x$ via the method `backward`.  More specifically, the `backward` function performs vector-Jacobian products, where the vector correspond to the `gradient` argument.  The key point in using the `backward` is thus to understand how to choose the `gradient` argument.\n",
    "\n",
    "The mathematical preliminaries above show how `gradient` should be chosen.  There are two key points:  \n",
    "1. `gradient` has the same shape as `y`;  \n",
    "1. `gradient` is populated with `0.`'s and `1.`'s, and the location of the `1.`'s corresponding to the inputs and outputs of interest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examples revisited"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Note:\n",
    "The variable `v` is passed to the `gradient` argument in all our examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the derivative of a scalar, univariate function evaluated a single point, we choose `gradient=torch.tensor(1.)`, which is the default value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of x         : ()\n",
      "Shape of y         : ()\n",
      "gradient argument  : 1.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.tensor(1., requires_grad=True)\n",
    "y = x**2\n",
    "v = torch.ones_like(y)\n",
    "y.backward()\n",
    "print(f\"Shape of x         : {tuple(x.shape)}\")\n",
    "print(f\"Shape of y         : {tuple(y.shape)}\")\n",
    "print(f\"gradient argument  : {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that if `x` is cast as a `1`-dimensional `tensor`, then (in this particular example) `y` is also a `1`-dimensional `tensor`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of x         : (1,)\n",
      "Shape of y         : (1,)\n",
      "gradient argument  : tensor([1.])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.tensor([1.], requires_grad=True)\n",
    "y = x**2\n",
    "v = torch.ones_like(y)\n",
    "y.backward()\n",
    "print(f\"Shape of x         : {tuple(x.shape)}\")\n",
    "print(f\"Shape of y         : {tuple(y.shape)}\")\n",
    "print(f\"gradient argument  : {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly if `x` is cast as `2`-dimensional `tensor`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of x         : (1, 1)\n",
      "Shape of y         : (1, 1)\n",
      "gradient argument  : tensor([[1.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.tensor([[1.]], requires_grad=True)\n",
    "y = x**2\n",
    "v = torch.ones_like(y)\n",
    "y.backward()\n",
    "print(f\"Shape of x         : {tuple(x.shape)}\")\n",
    "print(f\"Shape of y         : {tuple(y.shape)}\")\n",
    "print(f\"gradient argument  : {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the derivative of a scalar, univariate function evaluated at multiple points, `gradient` contains all `1.`'s and is of same shape as `y`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of x         : (5,)\n",
      "Shape of y         : (5,)\n",
      "gradient argument  : tensor([1., 1., 1., 1., 1.])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.linspace(-1, 1, 5, requires_grad=True)\n",
    "y = x**2\n",
    "v = torch.ones_like(y)\n",
    "y.backward(v)\n",
    "print(f\"Shape of x         : {tuple(x.shape)}\")\n",
    "print(f\"Shape of y         : {tuple(y.shape)}\")\n",
    "print(f\"gradient argument  : {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Casting `x` in a different shape changes the shape of `y`, and thus of `gradient`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of x         : (5, 1)\n",
      "Shape of y         : (5, 1)\n",
      "gradient argument  : \n",
      "tensor([[1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.linspace(-2, 2, 5).view(-1,1).requires_grad_(True)\n",
    "y = x**2\n",
    "v = torch.ones_like(y)\n",
    "y.backward(v)\n",
    "print(f\"Shape of x         : {tuple(x.shape)}\")\n",
    "print(f\"Shape of y         : {tuple(y.shape)}\")\n",
    "print(f\"gradient argument  : \")\n",
    "print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the derivative of a vector-valued, univariate function evaluated at a single point, the derivative of each component function is calculated one at a time, and `gradient` consists of all `0.`'s except for one `1.`, which is located at a position corresponding to the component function.  In the example below, the function is in fact *matrix-valued*, namely we calculate the derivative of\n",
    "$$f(x) = \\begin{bmatrix}1&x\\\\x^2&x^3\\\\x^4&x^5\\end{bmatrix}\\qquad \\textrm{at}\\quad x\\,=\\,1\\,.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x:\n",
      "tensor(1.)\n",
      "\n",
      "y:\n",
      "tensor([[1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.]])\n",
      "\n",
      "Derivatives:\n",
      "tensor([[0., 1.],\n",
      "        [2., 3.],\n",
      "        [4., 5.]])\n"
     ]
    }
   ],
   "source": [
    "# collapse-show\n",
    "import torch\n",
    "import itertools\n",
    "x = torch.tensor(1., requires_grad=True)\n",
    "y = torch.stack([x**i for i in range(6)]).view(3,2)\n",
    "ranges = [range(_) for _ in y.shape]\n",
    "\n",
    "print(\"x:\")\n",
    "print(x.data)\n",
    "print(\"\\ny:\")\n",
    "print(y.data)\n",
    "\n",
    "derivatives = torch.zeros_like(y)\n",
    "\n",
    "for i, j in itertools.product(*ranges):\n",
    "    v = torch.zeros_like(y)\n",
    "    v[i,j] = 1.\n",
    "    if x.grad is not None: x.grad.zero_()\n",
    "        \n",
    "    y.backward(v, retain_graph=True)\n",
    "    derivatives[i,j] = x.grad.item()\n",
    "print(\"\\nDerivatives:\")\n",
    "print(derivatives)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Note:\n",
    "The use of `for` loops can be avoided."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the gradient of a scalar, multivariate function evaluated at a single point, `gradient=torch.tensor(1.)`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of x         : (2,)\n",
      "Shape of y         : ()\n",
      "gradient argument  : 1.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.tensor([-1., 2.], requires_grad=True)\n",
    "w = torch.tensor([3., 5.])\n",
    "y = (x*x*w).sum()\n",
    "v = torch.ones_like(y)\n",
    "y.backward()\n",
    "print(f\"Shape of x         : {tuple(x.shape)}\")\n",
    "print(f\"Shape of y         : {tuple(y.shape)}\")\n",
    "print(f\"gradient argument  : {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following example, the input `x` is a `(3,2)`-tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of x: (3, 2)\n",
      "Shape of y: ()\n",
      "gradient argument: 1.0\n",
      "x:\n",
      "tensor([[0., 1.],\n",
      "        [2., 3.],\n",
      "        [4., 5.]], dtype=torch.float64)\n",
      "x.grad:\n",
      "tensor([[ 0.,  2.],\n",
      "        [ 4.,  6.],\n",
      "        [ 8., 10.]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "x = torch.arange(6, dtype=float).view(3,2).requires_grad_(True)\n",
    "y = (x**2).sum()\n",
    "v = torch.ones_like(y)\n",
    "y.backward(v)\n",
    "\n",
    "print(f\"Shape of x: {tuple(x.shape)}\")\n",
    "print(f\"Shape of y: {tuple(y.shape)}\")\n",
    "print(f\"gradient argument: {v}\")\n",
    "print(\"x:\")\n",
    "print(x.data)\n",
    "print(\"x.grad:\")\n",
    "print(x.grad.data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
